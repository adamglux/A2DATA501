---
title: "influenceR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{influenceR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: refs.bib
---

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(influenceR)
```

# influenceR: get to know your data 

The `influenceR` function, inside the `influenceR` package, calculates and evaluates key influence measures, including Cook's Distance, DFFITS, and Hadi's Influence, for linear or generalized linear models by computing diagnostic metrics to identify influential observations. The function has an optional setting that will generate plots to visualize these influence measures, aiding in the detection of data points that may impact the model's fit. This tool is helpful for model assessment and ensuring the reliability of statistical inferences.

## Background


### Cooks Distance 

In the Detection of Influential Observation in Linear Regression, Cook demonstrates,

>> A new measure based on confidence ellipsoids... for judging the contribution of each data point to the determination of the least squares estimate of the parameter vector in full rank linear regression models. It is shown that the measure combines information from the studentized residuals and the variances of the residuals and predicted values. [@cook_detection_1977]

In regression analysis, critical observations may skew, or effect the coefficients of estimation, and often these points need further analysis or deletion. Cook addresses the problem of determining which points are such of such critical value, by suggesting an interpretable measure that combines the variance of each residual, $\hat V(R_i)$, with studentised residuals, $t_i$, "(i.e. the residual divided by its standard error) have been recommended as more appropriate than the standardized residuals (i.e., the residual divided by the square root of the mean square for error) for detecting outliers." (@cook_detection_1977)

Studentised residual is given by:

$$t_i = [\frac{Y_i - \mathbf{x_i}'\hat \beta}{s\sqrt{1-v_i}}]$$

Where $v_i$ is the diagonal of the hat matrix (later given as $h_{ii}$). In the vector linear regression model, $\mathbf{Y} = \mathbf{X}\beta + \mathbf{e}$, residuals are given by: $\mathbf{R} = (R_i) = (\mathbf{I} - \mathbf{X}(\mathbf{X'}\mathbf{X})^{-1}\mathbf{X'})\mathbf{Y}$. 

Cook demonstrates that when we remove the ith point from the dataset, to produce $\mathbf{X}_{(i-1)}$, we get a measure of influence for the ith point in the vector of parameter estimates, $\hat \beta_{(-i)} = \hat \beta - \hat \beta_{i-1}$. Cook classifies this as when $D_i$ is greater than the median point of the $F_{p+1,(n-p-1)}$ distribution. This is measured by:^[note: *p* is the set of predictors of a full rank matrix, $\mathbf{X}$, and is sometimes given as $p + 1$, to indicate that the measure includes all parameters plus an intercept.]  

$$D_i = \frac{t_i^2}{p}\frac{V(\hat Y_i)}{V(R_i)}$$

Steps for calculating Cook's distance in R (assuming that model has been fit):

- Calculate regression coefficients, $\hat \beta = (X^TX)^{-1}X^Ty$
- calculate $\hat y_i = X\hat \beta$ and residuals $r_i = y_i - \hat y_i$
- get $\hat \sigma^2 = \frac{1}{n-p}\sum r^2_i$
- Obtain the Hat matrix: $H = X(X^TX)^{-1}X^T$
- Diagonal values are leverage values, $h_{ii}$
- Calculate studentised residuals $t_i = \frac{r_i}{s\sqrt{1-h_ii}}$, for $s = \sqrt{\frac{\sum e^2}{n-p}}$
- store any values greater |2| 
- Get $V(R_i) = (I - H)\sigma^2$, $V(\hat Y) = H\sigma^2$
- Calculate $D_i = \frac{t_i^2}{p}\frac{V(\hat Y_i)}{V(R_i)}$

### DFFITS

DFFITS is a diagnostic measure that assesses how much a fitted value for a point changes when that point is omitted from the model [@welsch_linear_1977]. DFFITS is similar to Cook's distance but provides a slightly different perspective by focusing on the change in predicted values rather than the overall fit of the model.

This measure is given by: [@uzuke_identifying_2021]

$$DFFITs_i = \frac{\hat y_j - \hat y_{j(i)}}{\hat\sigma_{(i)}\sqrt{h_{ii}}}$$

where $\hat y_j - \hat y_{j(i)} = t_i\sqrt{h_{ii}}$ and so $DFFITs_i = t_i\sqrt{\frac{h_{ii}}{1-h_{ii}}}$ 

To calculate DFFITS (assuming model is fit and Cook's distance has been calculated using the above):

- Use studentised residuals and leverage points calculated above to obtain DFFITS. 

### Hadi's Influcence

Hadi's measure is based on influential observations in the response or predictors, or both. 

From Hadi (1992), we get the following methodology: 

>We first order the n observations, using an appropriately chosen robust measure of outlyingness, then divide the data set into two initial subsets: A ‘basic’ subset which contains p +1 ‘good’ observations and a ‘non-basic’ subset which contains the remaining n - p - 1 observations. 

>Second, we compute the relative distance from each point in the data set to the centre of the basic subset, relative to the (possibly singular) covariance matrix of the basic subset. 

>Third, we rearrange the n observations in ascending order accordingly, then divide the data set into two subsets: A basic subset which contains the first p + 2 observations and a non-basic subset which contains the remaining n - p - 2 observations. 

>This process is repeated until an appropriately chosen stopping criterion is met. The final non-basic subset of observations is declared an outlying subset. 

[@hadi_identifying_1992]

The resulting equation is given by: 

$$H_i = \frac{h_{ii}}{1-h_{ii}} + \frac{p+1}{1-h_{ii}}\frac{d^2_i}{1-d^2_i}$$

Where $d_i = \frac{e_i}{\sqrt{SSE}}$. Here we have an additive function formed from the hat matrix with the a function of residuals. Large values for $H_i$ are influential given both the response and the predictor variables. 

### Influencial Observations

There are different methods for choosing a cut-off threshold for Cook's Distance. As $D_i \sim F_{p,n-p}$, cut off can used as the median point, $F_{0.5(p,n-p)}$ [@bollen_regression_1985]. For the test data, this value is close to 1 (0.8392304), however many sources use $\frac{4}{n}$ as a cut-off for identifying influential points, which for the test data was 0.02. 

As the purpose is to identify outliers, visually, $\frac{4}{n}$ will be used, with the conceit that $\approx 1$ is the bar for problematic points. 

For DFFITs, points that are $|DFFITs_i|\ge 3\sqrt{\frac{p+1}{(n-p-1)}}$ will be considered influential.

Kuh, and Welsch recommend 2 as a general cut-off value to indicate a value of influence. [@welsch_linear_1977]

## Remarks

Cook's distance, DFFITS, and Hadi's influence measure are all diagnostic tools used in regression analysis to identify influential observations—those data points that significantly affect the model's parameters or predictions.

All three techniques aim to detect influential data points by assessing the impact of each observation on the fitted regression model. They are particularly useful for identifying outliers or high-leverage points that could disproportionately skew the model’s results. Each method evaluates the effect of removing an observation on the estimated parameters or predicted values, helping to ensure the robustness of the model.

### Differences in each approach

**Cook's Distance** measures the influence of an observation by assessing how much the fitted values (predicted outcomes) change when that observation is removed. It combines information from both the residual (how far the point is from the fitted line) and the leverage (how much the point affects the estimation of the model coefficients). A commonly used rule of thumb is to consider observations with Cook’s Distance greater than $\frac{4}{n}$ or $F_{\alpha(p, n-p)}$ as potentially influential.

This measure is typically used for an overall assessment of an observation’s influence on the entire regression model. It’s useful to understand the combined effect of leverage and residuals on the model’s fitted values.

**DFFITS** (Difference in Fits) assesses the influence of an observation by evaluating the change in predicted values when that observation is removed. Unlike Cook’s Distance, which provides a single measure for each observation, DFFITS focuses on how the prediction for each observation changes when that observation is excluded. This makes DFFITS more sensitive to the specific impact of an observation on its own predicted value. The rule of thumb is that values greater than $\ge 3\sqrt{\frac{p+1}{(n-p-1)}}$ may indicate influential observations.

The method is more focused on the impact of each observation on its predicted value, making it useful when there is particular concerned about how individual data points affect their own predictions.

**Hadi’s Influence Measure** is designed to detect both high-leverage points and outliers by assessing the combined effect of an observation’s leverage and residual. Hadi’s measure is often considered more robust than Cook’s Distance and DFFITS because it accounts for the influence of both the leverage and the residuals simultaneously. A threshold value of 2 is often used to identify influential observations.

This method is used when a more comprehensive diagnostic is needed that considers both leverage and residuals, especially in situations where you suspect that the influence is due to both the observation's position in the design space and its discrepancy from the model.


## Installing the influenceR package

You can install the development version of `influenceR` from [GitHub](https://github.com/) with:

```
# install.packages("devtools")
devtools::install_github("adamglux/A2DATA501", build_vignettes = TRUE)
library(influenceR)
```

### suggested libraries  

```{r eval=FALSE, include=TRUE}
library(usethis)
library(devtools)
library(dplyr)
library(stats)
library(ggplot2)
library(gridExtra)
```

# Examples

```{r fig.height=3, fig.width=4.5}
#use data from mtcars
#plot all 
data1 <- mtcars
model1 <- lm(mpg ~ wt + hp, data = data1)
sample1 <- influenceR(model = model1, data = data1, plot = "all")

#show dataframe of influence measures 
head(sample1$influence_measures)

#output the dataframe by itself (no plots)
sample2 <- influenceR(model = model1, data = data1)
tail(sample2)

#show plots one by one 
sample1$plots$Cooks 
sample1$plots$DFFITS 
sample1$plots$Hadi 

# apply a theme to a grid of plots
#library(gridExtra)
#g2 <- lapply(sample1$plots, function(p) p+theme_minimal())
#grid.arrange(grobs = g2, ncol = 2, nrow = 2)
```

# References



